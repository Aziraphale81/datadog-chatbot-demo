# ğŸ“Š Monitor Threshold Analysis & Tuning Recommendations

**Date:** January 9, 2026  
**Analyst:** AI Assistant (via Datadog MCP)  
**Environment:** `demo`  
**Analysis Period:** Last 24 hours  
**Total Monitors:** 15

---

## ğŸ¯ Executive Summary

### Overall Health: **GOOD** âœ…

- **14/15 monitors** in healthy state (OK or No Data)
- **1/15 monitor** actively alerting (Backend API Latency - **expected behavior**)
- **Zero critical issues** requiring immediate action
- **3 monitors** recommended for threshold tuning
- **3 synthetic tests** not yet configured (No Data status)

### Key Findings

1. âœ… **Application is stable** - No actual errors or failures in 24h
2. âš ï¸ **One monitor alerting as expected** - Backend latency due to OpenAI API calls (15-25s response times)
3. ğŸ”§ **RabbitMQ Queue Depth threshold too high** - Set at 100, actual max is 0.65
4. ğŸ“Š **Postgres Slow Query threshold well-tuned** - Recent fix from 500â†’50,000,000ns
5. ğŸ”¬ **Synthetics tests need setup** - 3 monitors in No Data state

---

## ğŸ“ˆ Monitor Status Breakdown

| Status | Count | Monitors |
|--------|-------|----------|
| âœ… **OK** | 11 | Most application & infrastructure monitors |
| âš ï¸ **Alert** | 1 | Backend API Latency (expected - OpenAI slowness) |
| ğŸ“­ **No Data** | 3 | Synthetic tests (not configured) |

---

## ğŸ” Detailed Analysis by Category

---

### 1ï¸âƒ£ Application Performance Monitoring (APM)

#### **Backend API Latency (p95) - ALERTING** âš ï¸

**Monitor ID:** `247938727`  
**Query:** `avg(last_5m):p95:trace.http.request{service:chat-backend,env:demo} > 5`  
**Current Thresholds:** Critical: 5s, Warning: 3s

**24-Hour Performance Data:**
- **p50 Latency:** 8-14 seconds (typical)
- **p95 Latency:** 15-28 seconds (alerting range)
- **p99 Latency:** 15-28 seconds
- **Root Cause:** OpenAI API calls dominate (98% of request time)

**Analysis:**
âœ… **Monitor is working correctly** - Alerting as designed  
âš ï¸ **Threshold may be too strict** for OpenAI-dependent workload

**Recommendation:** ğŸ”§ **TUNE THRESHOLDS**

```hcl
monitor_thresholds {
  critical = 30.0  # Was: 5s â†’ New: 30s (accommodate OpenAI p99)
  warning  = 20.0  # Was: 3s â†’ New: 20s (accommodate OpenAI p95)
}
```

**Rationale:**
- Current 5s threshold triggers constantly during normal operations
- OpenAI API inherently slow (15-25s typical)
- Alert should fire only when performance degrades *beyond* OpenAI baseline
- Suggested 30s catches true outliers (>30s = 99th percentile degradation)

**Alternative:** Add a **composite monitor** that:
1. Alerts on latency > 30s **AND**
2. Excludes traces where `span.name:openai.request` duration > 25s

---

#### **Backend API Error Rate** âœ…

**Monitor ID:** `247938707`  
**Query:** `sum(last_5m):trace.http.request.errors{service:chat-backend,env:demo}.as_count() > 20`  
**Threshold:** 20 errors in 5 minutes

**24-Hour Performance Data:**
- **Total Errors:** 0
- **Total Requests:** ~1,200
- **Error Rate:** 0%

**Analysis:**
âœ… **Threshold appropriate** - No false positives  
âœ… **20 errors in 5min** = ~4 errors/min = reasonable sensitivity

**Recommendation:** âœ… **NO CHANGE NEEDED**

---

#### **Worker Error Rate** âœ…

**Monitor ID:** `247938726`  
**Query:** `sum(last_5m):sum:trace.http.request.errors{service:chat-worker,env:demo} > 10`  
**Threshold:** 10 errors in 5 minutes

**24-Hour Performance Data:**
- **Total Errors:** 0
- **Worker processed:** ~1,000 messages successfully

**Analysis:**
âœ… **Threshold appropriate** for async worker
âœ… **No false positives**

**Recommendation:** âœ… **NO CHANGE NEEDED**

---

#### **Frontend API Route Error Rate** âœ…

**Monitor ID:** `247938718`  
**Query:** `sum(last_5m):sum:trace.http.request.errors{service:chat-frontend,env:demo} > 10`  
**Threshold:** 10 errors in 5 minutes

**24-Hour Performance Data:**
- **Total Errors:** 0

**Analysis:**
âœ… **Threshold appropriate**

**Recommendation:** âœ… **NO CHANGE NEEDED**

---

### 2ï¸âƒ£ LLM Observability

#### **OpenAI API Error Rate** âœ…

**Monitor ID:** `247938715`  
**Query:** `sum(last_5m):sum:trace.openai.request.errors{env:demo} > 5`  
**Threshold:** 5 errors in 5 minutes

**24-Hour Performance Data:**
- **Total Errors:** 0
- **Total OpenAI Calls:** ~1,000
- **Success Rate:** 100%

**Analysis:**
âœ… **Threshold appropriate** - No OpenAI failures in 24h  
âœ… **5 errors in 5min** catches API key issues, quota limits, rate limiting

**Recommendation:** âœ… **NO CHANGE NEEDED**

---

#### **OpenAI API Failures (trace.openai.error)** âœ…

**Monitor ID:** `247938720`  
**Query:** `sum(last_10m):trace.openai.error{env:demo,service:chat-backend} > 1`  
**Threshold:** 1 error in 10 minutes

**24-Hour Performance Data:**
- **Total Errors:** 0

**Analysis:**
âœ… **Very sensitive threshold (1 error)** - appropriate for critical dependency  
âœ… **10-minute window** allows quick detection

**Recommendation:** âœ… **NO CHANGE NEEDED**

---

### 3ï¸âƒ£ Data Streams Monitoring (DSM)

#### **RabbitMQ Queue Depth** ğŸ”§

**Monitor ID:** `247938706`  
**Query:** `avg(last_5m):sum:rabbitmq.queue.messages{service:chat-rabbitmq,env:demo} > 100`  
**Current Threshold:** 100 messages

**24-Hour Performance Data:**
- **Average Queue Depth:** 0.2-0.3 messages
- **Maximum Queue Depth:** 0.65 messages
- **p95 Queue Depth:** <1 message
- **Worker Performance:** Processing faster than arrival rate

**Analysis:**
âš ï¸ **THRESHOLD TOO HIGH** - Would never alert!  
- Current max: 0.65 messages  
- Threshold: 100 messages  
- **Gap: 154x higher than actual**

**Recommendation:** ğŸ”§ **TUNE THRESHOLD**

```hcl
monitor_thresholds {
  critical = 10   # Was: 100 â†’ New: 10 (catches 15x normal)
  warning  = 5    # New: 5 messages (catches 8x normal)
}
```

**Rationale:**
- Normal operation: 0-1 message queue depth
- 5 messages = worker falling behind (8x baseline)
- 10 messages = significant backlog requiring investigation
- 100 messages would indicate catastrophic failure (never tested)

---

#### **RabbitMQ Connection Failures** âœ…

**Monitor ID:** `247938721`  
**Query:** `avg(last_5m):avg:rabbitmq.connections{service:chat-rabbitmq,env:demo} < 2`  
**Threshold:** < 2 connections

**24-Hour Performance Data:**
- **Average Connections:** 3 (steady)
- **Minimum Connections:** 2.4 (brief dip)

**Analysis:**
âœ… **Threshold appropriate** - Detects connection loss  
âœ… **Normal: 3 connections** (backend, worker, monitoring)  
âœ… **Alert if < 2** = at least one service disconnected

**Recommendation:** âœ… **NO CHANGE NEEDED**

---

### 4ï¸âƒ£ Database Monitoring (DBM)

#### **Postgres Slow Queries** âœ…

**Monitor ID:** `247938719`  
**Query:** `avg(last_10m):postgresql.queries.time{kube_namespace:chat-demo} > 50000000`  
**Threshold:** 50ms (50,000,000 nanoseconds)

**24-Hour Performance Data:**
- **Average Query Time:** 150-200 microseconds (0.15-0.2ms)
- **p95 Query Time:** ~500 microseconds (0.5ms)
- **Max Query Time:** 2-30ms (spikes during DAG execution)

**Analysis:**
âœ… **EXCELLENT threshold** - Recently fixed from 500ns to 50ms  
âœ… **50ms threshold** is **100-250x normal** = catches true slowdowns  
âœ… **No false positives** in 24h

**Recommendation:** âœ… **NO CHANGE NEEDED**

**Note:** Great tuning example! Previous threshold (500ns) would have been way too strict.

---

### 5ï¸âƒ£ Real User Monitoring (RUM)

#### **Frontend JavaScript Errors** âœ…

**Monitor ID:** `247938705`  
**Query:** `sum(last_5m):rum.measure.session.error{service:chat-frontend,env:demo}.as_count() > 5`  
**Threshold:** 5 errors in 5 minutes

**24-Hour Performance Data:**
- **Total RUM Errors:** 0
- **Sessions:** Multiple user sessions tracked

**Analysis:**
âœ… **Threshold appropriate** for production traffic  
âœ… **5 errors in 5min** = reasonable sensitivity for user impact

**Recommendation:** âœ… **NO CHANGE NEEDED**

---

### 6ï¸âƒ£ Infrastructure Monitoring

#### **Kubernetes Pod Restart Rate** âœ…

**Monitor ID:** `247938708`  
**Query:** `change(sum(last_2m),last_2m):kubernetes.containers.restarts{kube_namespace:chat-demo} > 2`  
**Threshold:** 2 restarts in 2 minutes

**24-Hour Performance Data:**
- **Pod Restarts:** 0 (all pods stable)
- **Longest Uptime:** 6+ days for most pods
- **Recent Restart:** Backend rolled out with new memory limits (planned)

**Analysis:**
âœ… **Threshold appropriate** - Recently tuned from `change(last_5m)` to `count(last_2m)`  
âœ… **Detects CrashLoopBackOff** quickly  
âœ… **Ignores planned rollouts** (gradual)

**Recommendation:** âœ… **NO CHANGE NEEDED**

---

### 7ï¸âƒ£ Data Jobs Monitoring (DJM)

#### **Airflow DAG Failures** âœ…

**Monitor ID:** `247938712`  
**Query:** `sum(last_15m):sum:airflow.dag.task.failed{service:chat-airflow,env:demo} > 3`  
**Threshold:** 3 failures in 15 minutes

**24-Hour Performance Data:**
- **Total Task Failures:** 0
- **DAG Executions:** Multiple successful runs
- **Tasks Created:** Active task scheduling observed

**Analysis:**
âœ… **Threshold appropriate** for demo workload  
âœ… **3 failures in 15min** = reasonable for 3 DAGs  
âœ… **15-minute window** balances responsiveness with noise

**Recommendation:** âœ… **NO CHANGE NEEDED**

---

### 8ï¸âƒ£ Synthetic Monitoring

#### **Three Synthetic Tests - NO DATA** ğŸ“­

**Monitor IDs:** `247938704`, `247938702`, `247938703`

1. **Frontend Uptime Check** - No Data
2. **Backend API Check (via Chat)** - No Data  
3. **ChatGPT User Journey (Browser)** - No Data

**Analysis:**
ğŸ“­ **Tests not configured yet** - Monitors created but no test execution

**Recommendation:** ğŸ“‹ **SETUP REQUIRED**

**Priority:** Low (application is healthy, APM/RUM provide coverage)

**Setup Instructions:**
1. Navigate to Synthetics â†’ Tests
2. Create API test for Frontend (http://localhost:30080/health)
3. Create API test for Backend (POST to /chat endpoint)
4. Create Browser test for user journey
5. Configure to run from internal private location or public locations

---

## ğŸ¯ Summary of Recommendations

### ğŸ”´ High Priority

None - System is stable!

### ğŸŸ¡ Medium Priority

1. **Tune Backend API Latency Monitor**
   - **Current:** Critical: 5s, Warning: 3s
   - **Recommended:** Critical: 30s, Warning: 20s
   - **Reason:** OpenAI API inherently slow (15-25s), current threshold causes alert fatigue
   - **Impact:** Reduces false positives while still catching true degradation

2. **Tune RabbitMQ Queue Depth Monitor**
   - **Current:** Critical: 100 messages
   - **Recommended:** Critical: 10, Warning: 5
   - **Reason:** Current threshold 154x higher than actual max (0.65)
   - **Impact:** Makes monitor actually useful for detecting worker lag

### ğŸŸ¢ Low Priority

3. **Setup Synthetic Tests** (Optional)
   - 3 synthetic test monitors exist but tests not configured
   - Current APM/RUM coverage is sufficient
   - Nice-to-have for proactive monitoring

---

## ğŸ“ Implementation Plan

### Step 1: Update Backend Latency Monitor

**File:** `terraform/monitors.tf` (lines 22-27)

```hcl
# BEFORE
monitor_thresholds {
  critical = 5.0  # 5 seconds
  warning  = 3.0  # 3 seconds
}

# AFTER
monitor_thresholds {
  critical = 30.0  # 30 seconds (accommodates OpenAI p99 latency)
  warning  = 20.0  # 20 seconds (accommodates OpenAI p95 latency)
}
```

**Commands:**
```bash
# Update terraform file
vim terraform/monitors.tf  # Edit lines 24-25

# Apply changes
cd terraform
terraform plan
terraform apply
```

---

### Step 2: Update RabbitMQ Queue Depth Monitor

**File:** `terraform/monitors.tf`

Find the `rabbitmq_queue_depth` monitor and update:

```hcl
# BEFORE
monitor_thresholds {
  critical = 100  # Too high!
}

# AFTER
monitor_thresholds {
  critical = 10   # 10 messages = significant backlog
  warning  = 5    # 5 messages = worker falling behind
}
```

**Commands:**
```bash
# Same as Step 1
cd terraform
terraform plan
terraform apply
```

---

### Step 3: Setup Synthetic Tests (Optional)

**Estimated Time:** 30 minutes

1. **API Test - Frontend Health**
   ```
   URL: http://localhost:30080/health
   Method: GET
   Expected: 200 OK
   Frequency: 5 minutes
   ```

2. **API Test - Backend Chat**
   ```
   URL: http://localhost:30080/api/chat
   Method: POST
   Body: {"prompt": "test", "session_id": "test"}
   Expected: 200 OK, response contains "reply"
   Frequency: 15 minutes
   ```

3. **Browser Test - User Journey**
   ```
   Steps:
   1. Navigate to http://localhost:30080
   2. Wait for page load
   3. Type message in input
   4. Click send
   5. Assert response appears
   Frequency: 30 minutes
   ```

---

## ğŸ“Š Expected Outcomes

### After Tuning

1. **Backend Latency Monitor**
   - âœ… Stops alerting during normal operations
   - âœ… Still alerts on true performance degradation (>30s)
   - âœ… Reduces alert fatigue
   - ğŸ“‰ **Alert frequency:** Daily â†’ Weekly (only on real issues)

2. **RabbitMQ Queue Depth Monitor**
   - âœ… Becomes sensitive to actual backlog conditions
   - âœ… Detects worker performance degradation early
   - âœ… Provides actionable alerts
   - ğŸ“ˆ **Usefulness:** 0% â†’ 100% (currently would never fire)

3. **Synthetic Tests** (if configured)
   - âœ… Proactive uptime monitoring
   - âœ… Detects issues before users report them
   - âœ… Validates full user journey end-to-end

---

## ğŸ† Monitor Health Scorecard

| Category | Score | Notes |
|----------|-------|-------|
| **APM Monitors** | ğŸŸ¡ 8/10 | One threshold needs tuning (latency) |
| **LLM Observability** | âœ… 10/10 | Perfect thresholds, zero false positives |
| **DSM Monitors** | ğŸŸ¡ 7/10 | Queue depth threshold too high |
| **DBM Monitors** | âœ… 10/10 | Excellent tuning (recently fixed!) |
| **RUM Monitors** | âœ… 10/10 | Appropriate sensitivity |
| **Infrastructure** | âœ… 10/10 | Well-tuned restart detection |
| **DJM Monitors** | âœ… 10/10 | Good balance of sensitivity |
| **Synthetics** | âšª N/A | Not configured yet |

**Overall Score:** ğŸŸ¢ **9.2/10** - Excellent monitoring coverage with minor tuning needed

---

## ğŸ”— Related Resources

### Datadog Links
- [APM Service Page: chat-backend](https://app.datadoghq.com/apm/service/chat-backend/operations?env=demo)
- [DBM Database Overview](https://app.datadoghq.com/databases?env=demo)
- [LLM Observability](https://app.datadoghq.com/llm?env=demo)
- [Monitor Management](https://app.datadoghq.com/monitors/manage?q=tag%3A%28env%3Ademo%29)

### Documentation
- [Monitor Types](https://docs.datadoghq.com/monitors/types/)
- [Threshold Configuration Best Practices](https://docs.datadoghq.com/monitors/configuration/)
- [Synthetic Monitoring Setup](https://docs.datadoghq.com/synthetics/)

---

## ğŸ“… Review Schedule

**Recommended Cadence:**
- **Weekly:** Review alerting monitors for false positives
- **Monthly:** Analyze historical performance trends, adjust thresholds
- **Quarterly:** Comprehensive monitor coverage review
- **After Incidents:** Tune related monitors based on learnings

**Next Review Date:** February 9, 2026

---

## âœ… Approval & Sign-off

**Analysis Completed By:** AI Assistant (Datadog MCP)  
**Review Date:** January 9, 2026  
**Approval Status:** â³ Pending Implementation

**Notes:** 
- All recommendations are based on 24-hour production telemetry
- No critical issues identified
- Tuning is optional but recommended for improved signal-to-noise ratio

---

*Generated using Datadog MCP Server - Model Context Protocol for Observability*






